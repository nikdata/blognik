<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>{reticulate} - A Tutorial | nikhil agarwal</title>
<meta name=keywords content="rstats,python">
<meta name=description content="A walkthrough on using {reticulate} to use the best of Python in R">
<meta name=author content="
">
<link rel=canonical href=https://nik.netlify.app/post/reticulate-tutorial/>
<link href=/assets/css/stylesheet.min.d08f6c302eba3eaecd589684c093cecdfe7dfb8745a207401bd0583d3bb31837.css integrity="sha256-0I9sMC66Pq7NWJaEwJPOzf59+4dFogdAG9BYPTuzGDc=" rel="preload stylesheet" as=style>
<link rel=icon href=https://nik.netlify.app/img/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://nik.netlify.app/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://nik.netlify.app/favicon-32x32.png>
<link rel=apple-touch-icon href=https://nik.netlify.app/apple-touch-icon.png>
<link rel=mask-icon href=https://nik.netlify.app/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<meta property="og:title" content="{reticulate} - A Tutorial">
<meta property="og:description" content="A walkthrough on using {reticulate} to use the best of Python in R">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nik.netlify.app/post/reticulate-tutorial/"><meta property="article:section" content="post">
<meta property="article:published_time" content="2021-01-27T00:00:00+00:00">
<meta property="article:modified_time" content="2021-01-27T00:00:00+00:00"><meta property="og:site_name" content="Nik">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="{reticulate} - A Tutorial">
<meta name=twitter:description content="A walkthrough on using {reticulate} to use the best of Python in R">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://nik.netlify.app/post/"},{"@type":"ListItem","position":3,"name":"{reticulate} - A Tutorial","item":"https://nik.netlify.app/post/reticulate-tutorial/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"{reticulate} - A Tutorial","name":"{reticulate} - A Tutorial","description":"A walkthrough on using {reticulate} to use the best of Python in R","keywords":["rstats","python"],"articleBody":"  With the recent release of RStudio 1.4, I’ve taken a new look at coding in Python using the RStudio IDE and also using Python libraries through {reticulate}.\nThe {reticulate} package is a clever package that enables you to ‘store’ Python libraries and ‘call’ Python functions using familiar R syntax. My daily workflow consists of wrangling \u0026 analyzing data with R. Lately, however, I’ve found that the Scikit-Learn library in Python has a much better ability to parallelize machine learning more easily than any other package in R. In a nutshell, I find that parallelizaton of machine learning routines in R is heavily dependent on the package itself. Scikit-Learn is essentially an ‘all-in-one’ solution for model development, but it is not the most intuitive to use (as compared to using {tidymodels}).\nHere’s a high level workflow for me:\n R  data acquisition from database with data wrangling and exploratory data analysis ‘tidy’ and clean data exported to CSV  Python  data preprocessing (e.g., balancing, train/test split) data modeling (hypertuning) with Scikit-Learn model scoring note down best model parameters  R  Apply best parameters to the appropriate model using the best package for the model (e.g., {ranger} for Random Forest) Construct API with {plumber} Deploy to production   This current workflow ‘forces’ me to rely on two different IDEs: RStudio for R and VSCode for Python. Prior to RStudio 1.4, it was difficult for me to know what objects were loaded into memory in the Environment tab as well as running Python scripts natively.\nWhile {reticulate} was great to use before RStudio 1.4, I still had to debug in VSCode any errors found with Python calls. Now with RStudio 1.4, I can do it all in one IDE.\nTL;DR This is a LONG post. You can find all the code used on my GitHub as a gist. This code will walk you through how I use {reticulate} effectively (i.e., combining the best of R and Python in one script).\n Pre-Requisites  RStudio 1.4 {tidyverse} {reticulate} {palmerpenguins} Python 3.8 or higher with virtual environments working knowledge of Python (I don’t talk much about Python decoding in this tutorial)  For this tutorial, I am assuming that you have already configured a particular virtual environment within the Global Options (or Project Options) menu.\nOne perk of using RStudio 1.4 is the fact that you no longer have to define the virtual environment in your R script once it is defined in the Global Options menu. However, you will need to do so if you wish to use a different virtual environment than the one in your Global Options (or Project Options) menu.\nPenguins!! As you may have guessed, I’ll be using the {palmerpenguins} package for the underlying data. The dataset within this package is far newer than the venerable iris dataset.\nThe objective is to predict the species of the penguin.\n Before we start I want you to keep in mind that this is a simple tutorial. I’m not trying to optimize the machine learning algorithm nor am I trying to code in the most efficient/concise manner. Rather, I’d like to ensure that the steps are easy to understand and follow along. As you apply these techniques in your own project, please feel free to optimize as needed.\n  Let’s get started! Let’s load the R libraries:\nlibrary(reticulate) library(palmerpenguins) library(dplyr) library(ggplot2) library(extrafont) library(tidyr) And now let’s load the appropriate Python libraries:\npysmote_over  Now let’s load the dataset (we’ll store in a dataframe called df_pgs for conciseness) and explore it.\ndf_pgs  ## Rows: 344 ## Columns: 8 ## $ species  Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… ## $ island  Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… ## $ bill_length_mm  39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … ## $ bill_depth_mm  18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … ## $ flipper_length_mm  181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… ## $ body_mass_g  3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … ## $ sex  male, female, female, NA, female, male, female, male… ## $ year  2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007… The data are encoded correctly if we wanted to build our model in R. For instance, the categorical columns are already encoded as factors. However, we need to remember that Scikit-Learn does not accept character values. Rather, these must be encoded as numerical values (either ordinally, dummy-encoded, or one-hot encoding). And we’ll do that when we get building a ‘recipe’.\nData Exploration - Missing Values One of the first things I like to do is identify if any columns have missing values.\ndf_pgs %% purrr::map_df(function(x) sum(is.na(x))) %% tidyr::pivot_longer(names_to = 'variable', cols = everything(), values_to = 'number_missing') %% dplyr::arrange(desc(number_missing)) ## # A tibble: 8 x 2 ## variable number_missing ##   ## 1 sex 11 ## 2 bill_length_mm 2 ## 3 bill_depth_mm 2 ## 4 flipper_length_mm 2 ## 5 body_mass_g 2 ## 6 species 0 ## 7 island 0 ## 8 year 0 Looks like 5 of the 8 columns have missing values. In fact, the column sex has 11 missing values. While it may be tempting to remove the missing values, I have found that ‘categorizing’ missing values in their own category can be ‘predictive’ in itself. Furthermore, numerical values can be imputed with a variety of methods. We’ll explore the imputing method more when we get to recipes.\n Data Exploration - Unique Values For our variables encoded as factors, let’s find out how many unique values we have. This will help us identify if it is necessary (or advisable) to group the categorizations for improved performance.\ndf_pgs %% dplyr::select_if(is.factor) %% purrr::map_df(function(x) n_distinct(x)) ## # A tibble: 1 x 3 ## species island sex ##    ## 1 3 3 3 This is nice! We don’t have too many unique values. In fact, the column sex has 3 unique values because the third value happens to be NA (which is ‘unknown’).\n Data Exploration - Check for Balance A key check is to ensure that the response variable is balanced (especially in a multi-classification problem).\ndf_pgs %% dplyr::group_by(species) %% count() ## # A tibble: 3 x 2 ## # Groups: species [3] ## species n ##   ## 1 Adelie 152 ## 2 Chinstrap 68 ## 3 Gentoo 124 And here we can see that there is a fairly large imbalance between the species Chinstrap and the other species. It’ll probably be a good idea to go ahead and balance the data. And we’ll do this balancing using the library imblearn in Python!\n Data Exploration - Visual Charts Let’s switch gears and take a look at some visual charts. First up: let’s compare bill dimensions by species.\ndf_pgs %% ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point(aes(color = species)) + scale_x_continuous(limits = c(30,60)) + scale_y_continuous(limits = c(12,22), breaks = seq(12,22,2)) + labs(x = \"Bill Length (mm)\", y = \"Bill Depth (mm)\", color = \"Species\", title = \"Bill Depth vs Length\", caption = \"Source: {palmerpenguins} package\") + theme(plot.title = element_text(family = \"Bahnschrift\", color = 'grey100'), plot.caption = element_text(family = \"Bahnschrift\", color = 'grey60'), axis.title = element_text(family = 'Bahnschrift', color = 'grey100'), axis.text.x = element_text(family = \"Bahnschrift\", color = 'grey70'), axis.text.y = element_text(family = \"Bahnschrift\", color = 'grey70'), plot.background = element_rect(fill = 'grey10'), panel.background = element_blank(), panel.grid.major = element_line(color = 'grey30', size = 0.2), panel.grid.minor = element_line(color = 'grey30', size = 0.2), legend.background = element_rect(fill = 'grey20'), legend.key = element_blank(), legend.title = element_text(family = 'Bahnschrift', color = 'grey80'), legend.text = element_text(family = \"Bahnschrift\", color = 'grey90'), legend.position = c(0.9, 0.2) ) ## Warning: Removed 2 rows containing missing values (geom_point). I have to admit that there is some really clear separation between the three groups. In the real world, this doesn’t happen often.\nSome key takeaways from this chart:\n The Adelie species tends to have shallower bill length however with greater bill depth. The Chinstrap species seems to be almost a ‘larger’ version of the Gentoo species. They have roughly the same range in bill length, but the Chinstraps have larger bill depths.  So let’s take a look at the weights of these species by gender.\ndf_pgs %% ggplot(aes(x = sex, y = body_mass_g)) + geom_boxplot(aes(fill = species), color = 'steelblue') + labs(x = \"Sex\", y = \"Body Mass (grams)\", title = \"Body Mass of Species by Sex\", caption = 'Source: {palmerpenguins} package') + theme(plot.title = element_text(family = \"Bahnschrift\", color = 'grey100'), plot.caption = element_text(family = \"Bahnschrift\", color = 'grey60'), axis.title = element_text(family = 'Bahnschrift', color = 'grey100'), axis.text.x = element_text(family = \"Bahnschrift\", color = 'grey70'), axis.text.y = element_text(family = \"Bahnschrift\", color = 'grey70'), plot.background = element_rect(fill = 'grey10'), panel.background = element_blank(), panel.grid.major = element_line(color = 'grey30', size = 0.2), panel.grid.minor = element_line(color = 'grey30', size = 0.2), legend.background = element_rect(fill = 'grey20'), legend.key = element_blank(), legend.title = element_text(family = 'Bahnschrift', color = 'grey80'), legend.text = element_text(family = \"Bahnschrift\", color = 'grey90'), legend.position = c(0.90, 0.9) ) ## Warning: Removed 2 rows containing non-finite values (stat_boxplot). Now this is interesting. The Gentoo species tends to be heavier than the other species for both genders. Interestingly, you’ll notice the NA category for Sex. Visually speaking, it appears that the missing values can be imputed with the appropriate gender based on weight. However, for this tutorial, we’ll call this column “unknown”.\n Data Pre-Processing Before we go any deeper, it’s probably a good idea to split the data into a ‘training’ and test’ group. This will enable us to assess the performance of any machine learning algorithm.\nset.seed(1337) pg_split  Recipes!!! Another intuitive tool within R is the ability to use the {recipes} package. Remarkably, this package enables you to create a step-by-step instruction on what to process for any dataframe that’s passed through it. While you can do similar things with Scikit-Learn, it’s just not as clean/crisp looking nor as intuitive. Don’t bother disputing me since you won’t convince me :) !\nbase_recipe % recipes::step_mutate(tmp_species = species) %% recipes::step_meanimpute(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, id = 'impute_missing_continuous') %% recipes::step_factor2string(sex) %% recipes::step_mutate(sex = ifelse(is.na(sex),'unknown',sex)) %% recipes::step_string2factor(sex, levels = c('male','female','unknown')) %% recipes::step_dummy(sex, one_hot = TRUE, levels = c('male','female','unknown'), preserve = TRUE) %% recipes::step_integer(species, strict = TRUE, id = 'label encode response variable') %% recipes::step_dummy(island, id = 'sparse encoding of islad variable', one_hot = TRUE, levels = c('Biscoe','Dream','Torgersen'), preserve = TRUE) %% recipes::prep(training = pg_train) base_recipe ## Data Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 7 ## ## Training data contained 258 data points and 5 incomplete rows. ## ## Operations: ## ## Variable mutation for tmp_species [trained] ## Mean Imputation for bill_length_mm, ... [trained] ## Character variables from sex [trained] ## Variable mutation for sex [trained] ## Factor variables from sex [trained] ## Dummy variables from sex [trained] ## Integer encoding for species [trained] ## Dummy variables from island [trained] There’s quite a bit going on, so let me help you decode it:\nI’m creating a new column called tmp_species so that I can preserve the ‘character string’ as it will be encoded as integers. This is more of a convenience thing as it will enable me to have a ‘decoder’. I’m imputing all missing values in continuous variables with the mean. I’m converting the values in column sex to a string so that I can more easily replace the NA values with ‘unknown’. Once that’s done, I convert it back to factors with the new level of ‘unknown’ and no more missing values. Since sex is a column with character values, I need to convert it to a sparse matrix for each potential value. Again, this is necessary for handling categorical values in Scikit-Learn. As previously mentioned, I then label encode each species (replacing the character string with a number). gain, this is necessary for handling categorical values in Scikit-Learn. And I finally construct a sparse matrix for the island column as well.  Since we’re using training data, the imputed value used in the training data will be the same value used in the test data.\nTo ‘build’ our modeling ready datasets, we need to ‘juice’ the training data and ‘bake’ the recipe to get the test data cleaned up. “Juicing” and “Baking” are taxonomies used in the {recipes} package to apply the recipe to datasets.\ncln_train  Let’s verify that we have no more missing data in either the training or test datasets.\ncln_train %% purrr::map_df(function(x) sum(is.na(x))) %% tidyr::pivot_longer(names_to = 'variable', cols = everything(), values_to = 'number_missing') ## # A tibble: 15 x 2 ## variable number_missing ##   ## 1 island 0 ## 2 bill_length_mm 0 ## 3 bill_depth_mm 0 ## 4 flipper_length_mm 0 ## 5 body_mass_g 0 ## 6 sex 0 ## 7 year 0 ## 8 species 0 ## 9 tmp_species 0 ## 10 sex_male 0 ## 11 sex_female 0 ## 12 sex_unknown 0 ## 13 island_Biscoe 0 ## 14 island_Dream 0 ## 15 island_Torgersen 0 cln_test %% purrr::map_df(function(x) sum(is.na(x))) %% tidyr::pivot_longer(names_to = 'variable', cols = everything(), values_to = 'number_missing') ## # A tibble: 15 x 2 ## variable number_missing ##   ## 1 island 0 ## 2 bill_length_mm 0 ## 3 bill_depth_mm 0 ## 4 flipper_length_mm 0 ## 5 body_mass_g 0 ## 6 sex 0 ## 7 year 0 ## 8 species 0 ## 9 tmp_species 0 ## 10 sex_male 0 ## 11 sex_female 0 ## 12 sex_unknown 0 ## 13 island_Biscoe 0 ## 14 island_Dream 0 ## 15 island_Torgersen 0 Note how we have no more missing values!\n Balancing Act Let’s check to see our class imbalance in the derived training dataframe.\ncln_train %% group_by(tmp_species) %% count() ## # A tibble: 3 x 2 ## # Groups: tmp_species [3] ## tmp_species n ##   ## 1 Adelie 108 ## 2 Chinstrap 59 ## 3 Gentoo 91 Notice how we don’t have comparable observations for each class. It is good practice to have a balanced dataset. However, such is not the case in life at times. Often, we can either up-sample or just down-sample. However, these fundamental approaches are not always effective. One of the techniques I use is SMOTE. I find SMOTE to be fairly well performing and it is my go to approach. I also find using SMOTE in Python easier than in R.\nLet’s touch base on a couple of nuances regarding many Python libraries:\n Many of them expect numerical values only. So you need to ensure that your data are encoded correctly. Many parameters/arguments expect integers. When using {reticulate}, you should explicitly specify integers with an “L” at the end (e.g., 2L).  # step 1: define the amount of samples desired over % select(-tmp_species, -island, -sex), y = cln_train %% select(tmp_species)) # step 4: extract the two different results and combine into one nb_pred  So let’s unpack what I’m doing:\nI’m defining the amount of samples I want for each class explicitly. In this instance, the actual arguments require a dictionary (key,value pair) for the specific classes. In this case, it’s far more easier to refer to them by name rather than the integer label encoding done prior. In this case, we are asking for 100 observations for each class. If the class has more than 100 samples already (such as Adelie), we will randomly under-sample. And vice-versa for the other classes.\n The SMOTE pipeline allows me to ‘daisy chain’ commands together so it can all run in one go. This is similar to the recipes concept.\n This is the step that executes the SMOTE algorithm on the supplied dataframe. Here is where {reticulate} shines as it ‘transcodes’ the R dataframe into something Python will understand. This step will return 2 dataframes: one for the predictors and the other for the response variable.\n Since we get back two distinct dataframes from the fit_resample() function, I convert them to a tibble and combine them together.\n  Now let’s see how many of each species we have in this new dataframe.\ncln_bal %% group_by(tmp_species) %% count() ## # A tibble: 3 x 2 ## # Groups: tmp_species [3] ## tmp_species n ##   ## 1 Adelie 100 ## 2 Chinstrap 100 ## 3 Gentoo 100 Now that looks amazing (and balanced)!\n  Model Development \u0026 Scoring To keep the overall post somewhat short (I’m already failing), I’ll only build a hyperparameter tuned Random Forest model. Nothing fancy, but just something to show how scikit-learn can be used with {reticulate}.\nFirst up, we define the hyperparameter tuning grid.\nparam_grid  Notice how I’m able to use a base R function seq() within a Python argument. Another beautiful feature of {reticulate}!\nOne nuance of Scikit-Learn is the fact that you have initialize a potential model engine. This is very much similar to the process found in {parsnip}.\nrf_mdl  In the code block above, I’ve identified the cross validation steps after initializing the model engine. Similar to how workflows work as part of {tidymodels}, all of this has been ‘stored’ in memory, but not actually executed. We have to ‘fit’ this engine to get the actual results.\ngrid_search$fit(X = cln_bal %% select(-tmp_species, -species), y = cln_bal$species) ## GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1, ## param_grid={'bootstrap': [True], 'max_depth': [1, 2, 3, 4], ## 'max_features': [2, 3, 4, 5, 6], ## 'min_samples_split': [2, 3, 4, 5], ## 'n_estimators': [250, 500]}, ## scoring='accuracy') One aspect of model fitting that doesn’t sit well with me is the fact that when models are fitted, you don’t have a visual insight on storing the model results. For instance, there is no  or = in the code block above. However, as that line runs, it will automatically store the results within grid_search.\nLet’s see what the best parameters are.\ngrid_search$best_params_ ## $bootstrap ## [1] TRUE ## ## $max_depth ## [1] 4 ## ## $max_features ## [1] 2 ## ## $min_samples_split ## [1] 2 ## ## $n_estimators ## [1] 500 From here on out, our process is simple:\n Make the predictions for the test dataset Evaluate the model (we’ll use accuracy)  # save the best combination best_grid = grid_search$best_estimator_ # make predictions ypred_test % select(-island, -tmp_species, -sex, -species)) # score sklearn_metrics$accuracy_score(y_true = cln_test$species, y_pred = ypred_test) ## [1] 0.9883721 And we get an accuracy of well over 98%. Not bad!\nI hope this brief tutorial helps you out! I’m a big fan of {reticulate} and I’ve been having tremendous fun using RStudio 1.4 to run my Python scripts natively along with R code.\n  ","wordCount":"3205","inLanguage":"en","datePublished":"2021-01-27T00:00:00Z","dateModified":"2021-01-27T00:00:00Z","author":{"@type":"Person","name":"Nik Agarwal"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nik.netlify.app/post/reticulate-tutorial/"},"publisher":{"@type":"Organization","name":"nikhil agarwal","logo":{"@type":"ImageObject","url":"https://nik.netlify.app/img/favicon.ico"}}}</script>
</head>
<body class=dark id=top>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://nik.netlify.app accesskey=h title="{nik} (Alt + H)">{nik}</a>
<span class=logo-switches>
</span>
</div>
<ul id=menu onscroll=menu_on_scroll()>
<li>
<a href=https://nik.netlify.app/post/ title=posts>
<span>posts</span>
</a>
</li>
<li>
<a href=https://nik.netlify.app/about/ title=about>
<span>about</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
{reticulate} - A Tutorial
</h1>
<div class=post-meta>
27 January 2021&nbsp;·
16 min
</div>
</header> <div class=toc>
<details open>
<summary accesskey=c title="(Alt + C)">
<div class=details>Table of Contents</div>
</summary>
<div class=inner><ul>
<li>
<a href=# aria-label=TL;DR>TL;DR</a></li>
<li>
<a href=# aria-label=Pre-Requisites>Pre-Requisites</a><ul>
<li>
<a href=# aria-label=Penguins!!>Penguins!!</a></li>
<li>
<a href=# aria-label="Before we start">Before we start</a></li></ul>
</li>
<li>
<a href=# aria-label="Let’s get started!">Let’s get started!</a><ul>
<li>
<a href=# aria-label="Data Exploration - Missing Values">Data Exploration - Missing Values</a></li>
<li>
<a href=# aria-label="Data Exploration - Unique Values">Data Exploration - Unique Values</a></li>
<li>
<a href=# aria-label="Data Exploration - Check for Balance">Data Exploration - Check for Balance</a></li>
<li>
<a href=# aria-label="Data Exploration - Visual Charts">Data Exploration - Visual Charts</a></li>
<li>
<a href=# aria-label="Data Pre-Processing">Data Pre-Processing</a><ul>
<li>
<a href=# aria-label=Recipes!!!>Recipes!!!</a></li>
<li>
<a href=# aria-label="Balancing Act">Balancing Act</a></li></ul>
</li>
<li>
<a href=# aria-label="Model Development &amp;amp; Scoring">Model Development & Scoring</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content>
<script src=https://nik.netlify.app/post/reticulate-tutorial/index_files/header-attrs/header-attrs.js></script>
<p>With the recent release of <a href=https://blog.rstudio.com/2021/01/19/announcing-rstudio-1-4/>RStudio 1.4</a>, I’ve taken a new look at coding in Python using the RStudio IDE and also using Python libraries through {reticulate}.</p>
<p>The {reticulate} package is a clever package that enables you to ‘store’ Python libraries and ‘call’ Python functions using familiar R syntax. My daily workflow consists of wrangling & analyzing data with R. Lately, however, I’ve found that the Scikit-Learn library in Python has a much better ability to parallelize machine learning more easily than any other package in R. In a nutshell, I find that parallelizaton of machine learning routines in R is heavily dependent on the package itself. Scikit-Learn is essentially an ‘all-in-one’ solution for model development, but it is not the most intuitive to use (as compared to using {tidymodels}).</p>
<p>Here’s a high level workflow for me:</p>
<ul>
<li>R
<ul>
<li>data acquisition from database with</li>
<li>data wrangling and exploratory data analysis</li>
<li>‘tidy’ and clean data exported to CSV</li>
</ul></li>
<li>Python
<ul>
<li>data preprocessing (e.g., balancing, train/test split)</li>
<li>data modeling (hypertuning) with Scikit-Learn</li>
<li>model scoring</li>
<li>note down best model parameters</li>
</ul></li>
<li>R
<ul>
<li>Apply best parameters to the appropriate model using the best package for the model (e.g., {ranger} for Random Forest)</li>
<li>Construct API with {plumber}</li>
<li>Deploy to production</li>
</ul></li>
</ul>
<p>This current workflow ‘forces’ me to rely on two different IDEs: RStudio for R and VSCode for Python. Prior to RStudio 1.4, it was difficult for me to know what objects were loaded into memory in the Environment tab as well as running Python scripts natively.</p>
<p>While {reticulate} was great to use before RStudio 1.4, I still had to debug in VSCode any errors found with Python calls. Now with RStudio 1.4, I can do it all in one IDE.</p>
<div id=tldr class="section level2">
<h2>TL;DR</h2>
<p>This is a LONG post. You can find all the code used on my <a href=https://gist.github.com/nikdata/ce4c72550f184e6bc20d6c98475e5a5d>GitHub as a gist</a>. This code will walk you through how I use {reticulate} effectively (i.e., combining the best of R and Python in one script).</p>
</div>
<div id=pre-requisites class="section level2">
<h2>Pre-Requisites</h2>
<ul>
<li>RStudio 1.4</li>
<li>{tidyverse}</li>
<li>{reticulate}</li>
<li>{palmerpenguins}</li>
<li>Python 3.8 or higher with virtual environments</li>
<li>working knowledge of Python (I don’t talk much about Python decoding in this tutorial)</li>
</ul>
<p>For this tutorial, I am assuming that you have already configured a particular virtual environment within the Global Options (or Project Options) menu.</p>
<p><img src=img/python-configured.png width=400></p>
<p>One perk of using RStudio 1.4 is the fact that you no longer have to define the virtual environment in your R script once it is defined in the Global Options menu. However, you will need to do so if you wish to use a different virtual environment than the one in your Global Options (or Project Options) menu.</p>
<div id=penguins class="section level3">
<h3>Penguins!!</h3>
<p>As you may have guessed, I’ll be using the {palmerpenguins} package for the underlying data. The dataset within this package is far newer than the venerable iris dataset.</p>
<p>The objective is to predict the species of the penguin.</p>
</div>
<div id=before-we-start class="section level3">
<h3>Before we start</h3>
<p>I want you to keep in mind that this is a simple tutorial. I’m not trying to optimize the machine learning algorithm nor am I trying to code in the most efficient/concise manner. Rather, I’d like to ensure that the steps are easy to understand and follow along. As you apply these techniques in your own project, please feel free to optimize as needed.</p>
</div>
</div>
<div id=lets-get-started class="section level2">
<h2>Let’s get started!</h2>
<p>Let’s load the R libraries:</p>
<pre class=r><code>library(reticulate)
library(palmerpenguins)
library(dplyr)
library(ggplot2)
library(extrafont)
library(tidyr)</code></pre>
<p>And now let’s load the appropriate Python libraries:</p>
<pre class=r><code>pysmote_over &lt;- reticulate::import(&#39;imblearn.over_sampling&#39;)
pysmote_under &lt;- reticulate::import(&#39;imblearn.under_sampling&#39;)
pysmote_pipe &lt;- reticulate::import(&#39;imblearn.pipeline&#39;)

sklearn_modelselection &lt;- reticulate::import(&#39;sklearn.model_selection&#39;)
sklearn_ensemble &lt;- reticulate::import(&#39;sklearn.ensemble&#39;)
sklearn_metrics &lt;- reticulate::import(&#39;sklearn.metrics&#39;)</code></pre>
<p>Now let’s load the dataset (we’ll store in a dataframe called <code>df_pgs</code> for conciseness) and explore it.</p>
<pre class=r><code>df_pgs &lt;- palmerpenguins::penguins

dplyr::glimpse(df_pgs)</code></pre>
<pre><code>## Rows: 344
## Columns: 8
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…
## $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …
## $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …
## $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…
## $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …
## $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…
## $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…</code></pre>
<p>The data are encoded correctly if we wanted to build our model in R. For instance, the categorical columns are already encoded as factors. However, we need to remember that Scikit-Learn does not accept character values. Rather, these must be encoded as numerical values (either ordinally, dummy-encoded, or one-hot encoding). And we’ll do that when we get building a ‘recipe’.</p>
<div id=data-exploration---missing-values class="section level3">
<h3>Data Exploration - Missing Values</h3>
<p>One of the first things I like to do is identify if any columns have missing values.</p>
<pre class=r><code>df_pgs %&gt;%
  purrr::map_df(function(x) sum(is.na(x))) %&gt;%
  tidyr::pivot_longer(names_to = &#39;variable&#39;, cols = everything(), values_to = &#39;number_missing&#39;) %&gt;%
  dplyr::arrange(desc(number_missing))</code></pre>
<pre><code>## # A tibble: 8 x 2
##   variable          number_missing
##   &lt;chr&gt;                      &lt;int&gt;
## 1 sex                           11
## 2 bill_length_mm                 2
## 3 bill_depth_mm                  2
## 4 flipper_length_mm              2
## 5 body_mass_g                    2
## 6 species                        0
## 7 island                         0
## 8 year                           0</code></pre>
<p>Looks like 5 of the 8 columns have missing values. In fact, the column <code>sex</code> has 11 missing values. While it may be tempting to remove the missing values, I have found that ‘categorizing’ missing values in their own category can be ‘predictive’ in itself. Furthermore, numerical values can be imputed with a variety of methods. We’ll explore the imputing method more when we get to recipes.</p>
</div>
<div id=data-exploration---unique-values class="section level3">
<h3>Data Exploration - Unique Values</h3>
<p>For our variables encoded as factors, let’s find out how many unique values we have. This will help us identify if it is necessary (or advisable) to group the categorizations for improved performance.</p>
<pre class=r><code>df_pgs %&gt;%
  dplyr::select_if(is.factor) %&gt;%
  purrr::map_df(function(x) n_distinct(x))</code></pre>
<pre><code>## # A tibble: 1 x 3
##   species island   sex
##     &lt;int&gt;  &lt;int&gt; &lt;int&gt;
## 1       3      3     3</code></pre>
<p>This is nice! We don’t have too many unique values. In fact, the column <code>sex</code> has 3 unique values because the third value happens to be NA (which is ‘unknown’).</p>
</div>
<div id=data-exploration---check-for-balance class="section level3">
<h3>Data Exploration - Check for Balance</h3>
<p>A key check is to ensure that the response variable is balanced (especially in a multi-classification problem).</p>
<pre class=r><code>df_pgs %&gt;%
  dplyr::group_by(species) %&gt;%
  count()</code></pre>
<pre><code>## # A tibble: 3 x 2
## # Groups:   species [3]
##   species       n
##   &lt;fct&gt;     &lt;int&gt;
## 1 Adelie      152
## 2 Chinstrap    68
## 3 Gentoo      124</code></pre>
<p>And here we can see that there is a fairly large imbalance between the species Chinstrap and the other species. It’ll probably be a good idea to go ahead and balance the data. And we’ll do this balancing using the library imblearn in Python!</p>
</div>
<div id=data-exploration---visual-charts class="section level3">
<h3>Data Exploration - Visual Charts</h3>
<p>Let’s switch gears and take a look at some visual charts. First up: let’s compare <code>bill</code> dimensions by species.</p>
<pre class=r><code>df_pgs %&gt;%
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(aes(color = species)) +
  scale_x_continuous(limits = c(30,60)) +
  scale_y_continuous(limits = c(12,22), breaks = seq(12,22,2)) +
  labs(x = &quot;Bill Length (mm)&quot;, 
       y = &quot;Bill Depth (mm)&quot;, 
       color = &quot;Species&quot;,
       title = &quot;Bill Depth vs Length&quot;,
       caption = &quot;Source: {palmerpenguins} package&quot;) +
  theme(plot.title = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey100&#39;),
        plot.caption = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey60&#39;),
        axis.title = element_text(family = &#39;Bahnschrift&#39;, color = &#39;grey100&#39;),
        axis.text.x = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey70&#39;),
        axis.text.y = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey70&#39;),
        plot.background = element_rect(fill = &#39;grey10&#39;),
        panel.background = element_blank(),
        panel.grid.major = element_line(color = &#39;grey30&#39;, size = 0.2),
        panel.grid.minor = element_line(color = &#39;grey30&#39;, size = 0.2),
        legend.background =  element_rect(fill = &#39;grey20&#39;),
        legend.key = element_blank(),
        legend.title = element_text(family = &#39;Bahnschrift&#39;, color = &#39;grey80&#39;),
        legend.text = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey90&#39;),
        legend.position = c(0.9, 0.2)
  )</code></pre>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_point).</code></pre>
<p><img src=https://nik.netlify.app/post/reticulate-tutorial/index_files/figure-html/chart1-1.png width=672></p>
<p>I have to admit that there is some really clear separation between the three groups. In the real world, this doesn’t happen often.</p>
<p>Some key takeaways from this chart:</p>
<ul>
<li>The Adelie species tends to have shallower bill length however with greater bill depth.</li>
<li>The Chinstrap species seems to be almost a ‘larger’ version of the Gentoo species. They have roughly the same range in bill length, but the Chinstraps have larger bill depths.</li>
</ul>
<p>So let’s take a look at the weights of these species by gender.</p>
<pre class=r><code>df_pgs %&gt;%
  ggplot(aes(x = sex, y = body_mass_g)) +
  geom_boxplot(aes(fill = species), color = &#39;steelblue&#39;) +
  labs(x = &quot;Sex&quot;, 
       y = &quot;Body Mass (grams)&quot;, 
       title = &quot;Body Mass of Species by Sex&quot;, 
       caption = &#39;Source: {palmerpenguins} package&#39;) +
  theme(plot.title = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey100&#39;),
        plot.caption = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey60&#39;),
        axis.title = element_text(family = &#39;Bahnschrift&#39;, color = &#39;grey100&#39;),
        axis.text.x = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey70&#39;),
        axis.text.y = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey70&#39;),
        plot.background = element_rect(fill = &#39;grey10&#39;),
        panel.background = element_blank(),
        panel.grid.major = element_line(color = &#39;grey30&#39;, size = 0.2),
        panel.grid.minor = element_line(color = &#39;grey30&#39;, size = 0.2),
        legend.background =  element_rect(fill = &#39;grey20&#39;),
        legend.key = element_blank(),
        legend.title = element_text(family = &#39;Bahnschrift&#39;, color = &#39;grey80&#39;),
        legend.text = element_text(family = &quot;Bahnschrift&quot;, color = &#39;grey90&#39;),
        legend.position = c(0.90, 0.9)
  )</code></pre>
<pre><code>## Warning: Removed 2 rows containing non-finite values (stat_boxplot).</code></pre>
<p><img src=https://nik.netlify.app/post/reticulate-tutorial/index_files/figure-html/chart2-1.png width=672></p>
<p>Now this is interesting. The Gentoo species tends to be heavier than the other species for both genders. Interestingly, you’ll notice the NA category for <code>Sex</code>. Visually speaking, it appears that the missing values can be imputed with the appropriate gender based on weight. However, for this tutorial, we’ll call this column “unknown”.</p>
</div>
<div id=data-pre-processing class="section level3">
<h3>Data Pre-Processing</h3>
<p>Before we go any deeper, it’s probably a good idea to split the data into a ‘training’ and test’ group. This will enable us to assess the performance of any machine learning algorithm.</p>
<pre class=r><code>set.seed(1337)
pg_split &lt;- rsample::initial_split(df_pgs, prop = 0.75)
pg_train &lt;- rsample::training(pg_split)
pg_test &lt;- rsample::testing(pg_split)</code></pre>
<div id=recipes class="section level4">
<h4>Recipes!!!</h4>
<p>Another intuitive tool within R is the ability to use the {recipes} package. Remarkably, this package enables you to create a step-by-step instruction on what to process for any dataframe that’s passed through it. While you can do similar things with Scikit-Learn, it’s just not as clean/crisp looking nor as intuitive. Don’t bother disputing me since you won’t convince me :) !</p>
<pre class=r><code>base_recipe &lt;- recipes::recipe(species ~ ., data = pg_train) %&gt;%
  recipes::step_mutate(tmp_species = species) %&gt;%
  recipes::step_meanimpute(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, id = &#39;impute_missing_continuous&#39;) %&gt;%
  recipes::step_factor2string(sex) %&gt;%
  recipes::step_mutate(sex = ifelse(is.na(sex),&#39;unknown&#39;,sex)) %&gt;%
  recipes::step_string2factor(sex, levels = c(&#39;male&#39;,&#39;female&#39;,&#39;unknown&#39;)) %&gt;%
  recipes::step_dummy(sex, one_hot = TRUE, levels = c(&#39;male&#39;,&#39;female&#39;,&#39;unknown&#39;), preserve = TRUE) %&gt;%
  recipes::step_integer(species, strict = TRUE, id = &#39;label encode response variable&#39;) %&gt;%
  recipes::step_dummy(island, id = &#39;sparse encoding of islad variable&#39;, one_hot = TRUE, levels = c(&#39;Biscoe&#39;,&#39;Dream&#39;,&#39;Torgersen&#39;), preserve = TRUE) %&gt;%
  recipes::prep(training = pg_train)

base_recipe</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          7
## 
## Training data contained 258 data points and 5 incomplete rows. 
## 
## Operations:
## 
## Variable mutation for tmp_species [trained]
## Mean Imputation for bill_length_mm, ... [trained]
## Character variables from sex [trained]
## Variable mutation for sex [trained]
## Factor variables from sex [trained]
## Dummy variables from sex [trained]
## Integer encoding for species [trained]
## Dummy variables from island [trained]</code></pre>
<p>There’s quite a bit going on, so let me help you decode it:</p>
<ol style=list-style-type:decimal>
<li>I’m creating a new column called tmp_species so that I can preserve the ‘character string’ as it will be encoded as integers. This is more of a convenience thing as it will enable me to have a ‘decoder’.</li>
<li>I’m imputing all missing values in continuous variables with the mean.</li>
<li>I’m converting the values in column <code>sex</code> to a string so that I can more easily replace the NA values with ‘unknown’. Once that’s done, I convert it back to factors with the new level of ‘unknown’ and no more missing values.</li>
<li>Since <code>sex</code> is a column with character values, I need to convert it to a sparse matrix for each potential value. Again, this is necessary for handling categorical values in Scikit-Learn.</li>
<li>As previously mentioned, I then label encode each species (replacing the character string with a number). gain, this is necessary for handling categorical values in Scikit-Learn.</li>
<li>And I finally construct a sparse matrix for the <code>island</code> column as well.</li>
</ol>
<p>Since we’re using training data, the imputed value used in the training data will be the same value used in the test data.</p>
<p>To ‘build’ our modeling ready datasets, we need to ‘juice’ the training data and ‘bake’ the recipe to get the test data cleaned up. “Juicing” and “Baking” are taxonomies used in the {recipes} package to apply the recipe to datasets.</p>
<pre class=r><code>cln_train &lt;- recipes::juice(base_recipe)
cln_test &lt;- recipes::bake(base_recipe, pg_test)</code></pre>
<p>Let’s verify that we have no more missing data in either the training or test datasets.</p>
<pre class=r><code>cln_train %&gt;%
  purrr::map_df(function(x) sum(is.na(x))) %&gt;%
  tidyr::pivot_longer(names_to = &#39;variable&#39;, cols = everything(), values_to = &#39;number_missing&#39;)</code></pre>
<pre><code>## # A tibble: 15 x 2
##    variable          number_missing
##    &lt;chr&gt;                      &lt;int&gt;
##  1 island                         0
##  2 bill_length_mm                 0
##  3 bill_depth_mm                  0
##  4 flipper_length_mm              0
##  5 body_mass_g                    0
##  6 sex                            0
##  7 year                           0
##  8 species                        0
##  9 tmp_species                    0
## 10 sex_male                       0
## 11 sex_female                     0
## 12 sex_unknown                    0
## 13 island_Biscoe                  0
## 14 island_Dream                   0
## 15 island_Torgersen               0</code></pre>
<pre class=r><code>cln_test %&gt;%
  purrr::map_df(function(x) sum(is.na(x))) %&gt;%
  tidyr::pivot_longer(names_to = &#39;variable&#39;, cols = everything(), values_to = &#39;number_missing&#39;)</code></pre>
<pre><code>## # A tibble: 15 x 2
##    variable          number_missing
##    &lt;chr&gt;                      &lt;int&gt;
##  1 island                         0
##  2 bill_length_mm                 0
##  3 bill_depth_mm                  0
##  4 flipper_length_mm              0
##  5 body_mass_g                    0
##  6 sex                            0
##  7 year                           0
##  8 species                        0
##  9 tmp_species                    0
## 10 sex_male                       0
## 11 sex_female                     0
## 12 sex_unknown                    0
## 13 island_Biscoe                  0
## 14 island_Dream                   0
## 15 island_Torgersen               0</code></pre>
<p>Note how we have no more missing values!</p>
</div>
<div id=balancing-act class="section level4">
<h4>Balancing Act</h4>
<p>Let’s check to see our class imbalance in the derived training dataframe.</p>
<pre class=r><code>cln_train %&gt;%
  group_by(tmp_species) %&gt;%
  count()</code></pre>
<pre><code>## # A tibble: 3 x 2
## # Groups:   tmp_species [3]
##   tmp_species     n
##   &lt;fct&gt;       &lt;int&gt;
## 1 Adelie        108
## 2 Chinstrap      59
## 3 Gentoo         91</code></pre>
<p>Notice how we don’t have comparable observations for each class. It is good practice to have a balanced dataset. However, such is not the case in life at times. Often, we can either up-sample or just down-sample. However, these fundamental approaches are not always effective. One of the techniques I use is <a href=https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106>SMOTE</a>. I find SMOTE to be fairly well performing and it is my go to approach. I also find using SMOTE in Python easier than in R.</p>
<p>Let’s touch base on a couple of nuances regarding many Python libraries:</p>
<ul>
<li>Many of them expect numerical values only. So you need to ensure that your data are encoded correctly.</li>
<li>Many parameters/arguments expect integers. When using {reticulate}, you should explicitly specify integers with an “L” at the end (e.g., 2L).</li>
</ul>
<pre class=r><code># step 1: define the amount of samples desired
over &lt;- pysmote_over$SMOTE(sampling_strategy = reticulate::dict(&#39;Chinstrap&#39; = 100L, &#39;Gentoo&#39; = 100L), random_state = 1337L)
under &lt;- pysmote_under$RandomUnderSampler(sampling_strategy = reticulate::dict(&#39;Adelie&#39; = 100L), random_state = 1234L)

# step 2: define pipeline (similar to recipes in R) to under and over sample in one go
steps &lt;- list(c(&#39;o&#39;, over), c(&#39;u&#39;, under))
pipeline &lt;- pysmote_pipe$Pipeline(steps = steps)

# step 3: do the actual resampling
df_rebal &lt;- pipeline$fit_resample(X = cln_train %&gt;% select(-tmp_species, -island, -sex), y = cln_train %&gt;% select(tmp_species))

# step 4: extract the two different results and combine into one
nb_pred &lt;- as_tibble(df_rebal[[1]])
nb_resp &lt;- as_tibble(df_rebal[[2]])

cln_bal &lt;- bind_cols(nb_resp, nb_pred)</code></pre>
<p>So let’s unpack what I’m doing:</p>
<ol style=list-style-type:decimal>
<li><p>I’m defining the amount of samples I want for each class explicitly. In this instance, the actual arguments require a dictionary (key,value pair) for the specific classes. In this case, it’s far more easier to refer to them by name rather than the integer label encoding done prior. In this case, we are asking for 100 observations for each class. If the class has more than 100 samples already (such as Adelie), we will randomly under-sample. And vice-versa for the other classes.</p></li>
<li><p>The SMOTE pipeline allows me to ‘daisy chain’ commands together so it can all run in one go. This is similar to the recipes concept.</p></li>
<li><p>This is the step that executes the SMOTE algorithm on the supplied dataframe. Here is where {reticulate} shines as it ‘transcodes’ the R dataframe into something Python will understand. This step will return 2 dataframes: one for the predictors and the other for the response variable.</p></li>
<li><p>Since we get back two distinct dataframes from the <code>fit_resample()</code> function, I convert them to a tibble and combine them together.</p></li>
</ol>
<p>Now let’s see how many of each species we have in this new dataframe.</p>
<pre class=r><code>cln_bal %&gt;%
  group_by(tmp_species) %&gt;%
  count()</code></pre>
<pre><code>## # A tibble: 3 x 2
## # Groups:   tmp_species [3]
##   tmp_species     n
##   &lt;fct&gt;       &lt;int&gt;
## 1 Adelie        100
## 2 Chinstrap     100
## 3 Gentoo        100</code></pre>
<p>Now that looks amazing (and balanced)!</p>
</div>
</div>
<div id=model-development-scoring class="section level3">
<h3>Model Development & Scoring</h3>
<p>To keep the overall post somewhat short (I’m already failing), I’ll only build a hyperparameter tuned Random Forest model. Nothing fancy, but just something to show how scikit-learn can be used with {reticulate}.</p>
<p>First up, we define the hyperparameter tuning grid.</p>
<pre class=r><code>param_grid &lt;- reticulate::dict(&#39;bootstrap&#39; = list(TRUE),
                               &#39;max_depth&#39; = seq(1L,4L,1L),
                               &#39;max_features&#39; = seq(2L,6L,1L),
                               &#39;min_samples_split&#39; = seq(2L,5L,1L),
                               &#39;n_estimators&#39; = c(250L, 500L))</code></pre>
<p>Notice how I’m able to use a base R function <code>seq()</code> within a Python argument. Another beautiful feature of {reticulate}!</p>
<p>One nuance of Scikit-Learn is the fact that you have initialize a potential model engine. This is very much similar to the process found in {parsnip}.</p>
<pre class=r><code>rf_mdl &lt;- sklearn_ensemble$RandomForestClassifier()

grid_search &lt;- sklearn_modelselection$GridSearchCV(estimator = rf_mdl,
                                                   param_grid = param_grid,
                                                   cv = 5L,
                                                   n_jobs = -1L,
                                                   verbose = 0L,
                                                   scoring = &#39;accuracy&#39;,
                                                   refit = TRUE)</code></pre>
<p>In the code block above, I’ve identified the cross validation steps after initializing the model engine. Similar to how workflows work as part of {tidymodels}, all of this has been ‘stored’ in memory, but not actually executed. We have to ‘fit’ this engine to get the actual results.</p>
<pre class=r><code>grid_search$fit(X = cln_bal %&gt;% select(-tmp_species, -species), y = cln_bal$species)</code></pre>
<pre><code>## GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,
##              param_grid={&#39;bootstrap&#39;: [True], &#39;max_depth&#39;: [1, 2, 3, 4],
##                          &#39;max_features&#39;: [2, 3, 4, 5, 6],
##                          &#39;min_samples_split&#39;: [2, 3, 4, 5],
##                          &#39;n_estimators&#39;: [250, 500]},
##              scoring=&#39;accuracy&#39;)</code></pre>
<p>One aspect of model fitting that doesn’t sit well with me is the fact that when models are fitted, you don’t have a visual insight on storing the model results. For instance, there is no <code>&lt;-</code> or <code>=</code> in the code block above. However, as that line runs, it will automatically store the results within <code>grid_search</code>.</p>
<p>Let’s see what the best parameters are.</p>
<pre class=r><code>grid_search$best_params_</code></pre>
<pre><code>## $bootstrap
## [1] TRUE
## 
## $max_depth
## [1] 4
## 
## $max_features
## [1] 2
## 
## $min_samples_split
## [1] 2
## 
## $n_estimators
## [1] 500</code></pre>
<p>From here on out, our process is simple:</p>
<ul>
<li>Make the predictions for the test dataset</li>
<li>Evaluate the model (we’ll use accuracy)</li>
</ul>
<pre class=r><code># save the best combination
best_grid = grid_search$best_estimator_

# make predictions
ypred_test &lt;- best_grid$predict(cln_test %&gt;% select(-island, -tmp_species, -sex, -species))

# score
sklearn_metrics$accuracy_score(y_true = cln_test$species, y_pred = ypred_test)</code></pre>
<pre><code>## [1] 0.9883721</code></pre>
<p>And we get an accuracy of well over 98%. Not bad!</p>
<p>I hope this brief tutorial helps you out! I’m a big fan of {reticulate} and I’ve been having tremendous fun using RStudio 1.4 to run my Python scripts natively along with R code.</p>
</div>
</div>
</div>
<footer class=post-footer>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://nik.netlify.app>nikhil agarwal</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)">
<button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script defer src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
</body>
</html>